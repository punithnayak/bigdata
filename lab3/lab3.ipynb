{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n",
      "+---+--------------------+\n",
      "| id|         description|\n",
      "+---+--------------------+\n",
      "|  1|This is an exampl...|\n",
      "|  2|Another example w...|\n",
      "+---+--------------------+\n",
      "\n",
      "Data after Tokenization:\n",
      "+--------------------------------------+--------------------------------------------+\n",
      "|description                           |tokens                                      |\n",
      "+--------------------------------------+--------------------------------------------+\n",
      "|This is an example sentence.          |[this, is, an, example, sentence.]          |\n",
      "|Another example with some punctuation!|[another, example, with, some, punctuation!]|\n",
      "+--------------------------------------+--------------------------------------------+\n",
      "\n",
      "Data after Normalization:\n",
      "+--------------------------------------+-------------------------------------+\n",
      "|description                           |normalized_description               |\n",
      "+--------------------------------------+-------------------------------------+\n",
      "|This is an example sentence.          |this is an example sentence          |\n",
      "|Another example with some punctuation!|another example with some punctuation|\n",
      "+--------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EntityResolutionPreprocessing\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1\", \"This is an example sentence.\"),\n",
    "        (\"2\", \"Another example with some punctuation!\")]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"description\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the initial data\n",
    "print(\"Initial Data:\")\n",
    "df.show()\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Display data after tokenization\n",
    "print(\"Data after Tokenization:\")\n",
    "df.select(\"description\", \"tokens\").show(truncate=False)\n",
    "\n",
    "# Step 2: Normalization (lowercasing and removing special characters)\n",
    "df = df.withColumn(\"normalized_description\", lower(col(\"description\")))\n",
    "df = df.withColumn(\"normalized_description\", regexp_replace(\"normalized_description\", \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "# Display data after normalization\n",
    "print(\"Data after Normalization:\")\n",
    "df.select(\"description\", \"normalized_description\").show(truncate=False)\n",
    "\n",
    "# Additional preprocessing steps can be added based on your requirements.\n",
    "\n",
    "# Finally, perform entity resolution using the preprocessed data.\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|        description|\n",
      "+---+-------------------+\n",
      "|  1|apple orange banana|\n",
      "|  2| orange banana kiwi|\n",
      "|  3|  pear apple banana|\n",
      "+---+-------------------+\n",
      "\n",
      "Similarity Scores:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/19 14:25:13 WARN SimpleFunctionRegistry: The function jaccard_similarity replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+--------------------+------------------+\n",
      "|id1|id2|              words1|              words2|jaccard_similarity|\n",
      "+---+---+--------------------+--------------------+------------------+\n",
      "|  1|  2|[apple, orange, b...|[orange, banana, ...|               0.5|\n",
      "|  1|  3|[apple, orange, b...|[pear, apple, ban...|               0.5|\n",
      "|  2|  3|[orange, banana, ...|[pear, apple, ban...|               0.2|\n",
      "+---+---+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, expr\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SimilarityScores\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1\", \"apple orange banana\"),\n",
    "        (\"2\", \"orange banana kiwi\"),\n",
    "        (\"3\", \"pear apple banana\")]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"description\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the initial data\n",
    "print(\"Initial Data:\")\n",
    "df.show()\n",
    "\n",
    "# Split the description into a list of words\n",
    "df = df.withColumn(\"words\", split(col(\"description\"), \" \"))\n",
    "\n",
    "# Define a function to compute Jaccard similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection_size = len(set1 & set2)\n",
    "    union_size = len(set1 | set2)\n",
    "    return intersection_size / union_size if union_size != 0 else 0.0\n",
    "\n",
    "# Define a UDF for the Jaccard similarity function\n",
    "spark.udf.register(\"jaccard_similarity\", jaccard_similarity)\n",
    "\n",
    "# Compute Jaccard similarity scores between records\n",
    "df_similarity = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.id\") < col(\"df2.id\")) \\\n",
    "    .select(col(\"df1.id\").alias(\"id1\"),\n",
    "            col(\"df2.id\").alias(\"id2\"),\n",
    "            col(\"df1.words\").alias(\"words1\"),\n",
    "            col(\"df2.words\").alias(\"words2\"),\n",
    "            expr(\"jaccard_similarity(words1, words2)\").alias(\"jaccard_similarity\"))\n",
    "\n",
    "# Display similarity scores\n",
    "print(\"Similarity Scores:\")\n",
    "df_similarity.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a PySpark program to evaluate the precision, recall, and F1-score of an entity\n",
    "resolution model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/19 14:29:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.75\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 1.0\n",
      "F1-Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Tokenizer, NGram, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EntityResolutionEvaluation\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1\", \"John Doe\", \"123 Main St\", \"john.doe@email.com\"),\n",
    "        (\"2\", \"Jane Smith\", \"456 Oak Ave\", \"jane.smith@email.com\"),\n",
    "        (\"3\", \"John Doe\", \"789 Pine St\", \"john.doe@email.com\"),\n",
    "        (\"4\", \"Jake Brown\", \"101 Elm St\", \"jake.brown@email.com\")]\n",
    "\n",
    "# Ground truth data (1 if match, 0 if non-match)\n",
    "ground_truth_data = [(\"1\", \"2\", 0),\n",
    "                     (\"1\", \"3\", 1),\n",
    "                     (\"1\", \"4\", 0),\n",
    "                     (\"2\", \"3\", 0),\n",
    "                     (\"2\", \"4\", 0),\n",
    "                     (\"3\", \"4\", 0)]\n",
    "\n",
    "# Define the schema\n",
    "data_schema = [\"id\", \"name\", \"address\", \"email\"]\n",
    "ground_truth_schema = [\"id1\", \"id2\", \"label\"]\n",
    "\n",
    "# Create DataFrames\n",
    "df = spark.createDataFrame(data, schema=data_schema)\n",
    "ground_truth_df = spark.createDataFrame(ground_truth_data, schema=ground_truth_schema)\n",
    "\n",
    "# Add a label column to the 'df' DataFrame based on matching names\n",
    "df_labeled = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.name\") == col(\"df2.name\")) \\\n",
    "    .select(col(\"df1.id\").alias(\"id1\"),\n",
    "            col(\"df2.id\").alias(\"id2\"),\n",
    "            col(\"df1.name\").alias(\"name\"),\n",
    "            when(col(\"df1.id\") == col(\"df2.id\"), 1).otherwise(0).alias(\"label\"))\n",
    "\n",
    "# Tokenize and normalize columns for matching\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"name_tokens\")\n",
    "ngram = NGram(n=2, inputCol=\"name_tokens\", outputCol=\"name_ngrams\")\n",
    "vectorizer = CountVectorizer(inputCol=\"name_ngrams\", outputCol=\"name_vector\")\n",
    "\n",
    "# Create a simple entity resolution model (using Logistic Regression as an example)\n",
    "lr = LogisticRegression(featuresCol=\"name_vector\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram, vectorizer, lr])\n",
    "\n",
    "# Fit the model\n",
    "model = pipeline.fit(df_labeled)  # Use the labeled DataFrame 'df_labeled' for fitting the pipeline\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(df_labeled)  # Use the labeled DataFrame 'df_labeled' for making predictions\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "area_under_roc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Compute precision, recall, and F1-score\n",
    "tp = predictions.filter((col(\"label\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "fp = predictions.filter((col(\"label\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "fn = predictions.filter((col(\"label\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Area under ROC:\", area_under_roc)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
