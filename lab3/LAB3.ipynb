{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|  _c0|  _c1|              _c2|         _c3|         _c4|         _c5|    _c6|   _c7|   _c8|   _c9|   _c10|    _c11|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "|37291|53113|0.833333333333333|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and set up Spark session\n",
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()\n",
    "\n",
    "# Read data from CSV files\n",
    "prev = spark.read.option(\"recursiveFileLookup\", \"true\").csv(\"donation/*.csv\")\n",
    "\n",
    "# Show a sample of the data\n",
    "prev.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+------------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|lname_tokens|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+------------+\n",
      "|37291|53113|0.833333333333333|        null|           1|        null|      1|     1|     1|     1|      0|    true|         [1]|\n",
      "|39086|47614|                1|        null|           1|        null|      1|     1|     1|     1|      1|    true|         [1]|\n",
      "|70031|70237|                1|        null|           1|        null|      1|     1|     1|     1|      1|    true|         [1]|\n",
      "|84795|97439|                1|        null|           1|        null|      1|     1|     1|     1|      1|    true|         [1]|\n",
      "|36950|42116|                1|        null|           1|           1|      1|     1|     1|     1|      1|    true|         [1]|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define a UDF for lowercasing each element in the array\n",
    "lowercase_udf = udf(lambda tokens: [token.lower() for token in tokens], ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the lname_tokens column\n",
    "parsed = parsed.withColumn(\"lname_tokens\", lowercase_udf(col(\"lname_tokens\")))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "|    null|      1|\n",
      "+--------+-------+\n",
      "\n",
      "+--------+-------+\n",
      "|is_match|    cnt|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "|    null|      1|\n",
      "+--------+-------+\n",
      "\n",
      "+-------+--------------------+------------------+\n",
      "|summary|        cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|             5748126|            103699|\n",
      "|   mean|  0.7129023464241683|0.9000089989364238|\n",
      "| stddev|  0.3887584395082916|0.2713306768152377|\n",
      "|    min|                   0|                 0|\n",
      "|    max|2.68694413843136e-05|                 1|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform data analysis and compute similarity scores\n",
    "from pyspark.sql.functions import col\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "parsed.createOrReplaceTempView(\"linkage\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT is_match, COUNT(*) cnt\n",
    "    FROM linkage\n",
    "    GROUP BY is_match\n",
    "    ORDER BY cnt DESC\n",
    "\"\"\").show()\n",
    "\n",
    "summary = parsed.describe()\n",
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n",
    "\n",
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|score|is_match|\n",
      "+-----+--------+\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| null|    6|    789|\n",
      "| true|20871|    637|\n",
      "|false|   54|5726775|\n",
      "+-----+-----+-------+\n",
      "\n",
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| null|    6|    789|\n",
      "| true|20925| 596413|\n",
      "|false| null|5130999|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9703831132601822, 0.9974193548387097, 0.9837155044422973)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Assuming 'parsed' DataFrame is already defined\n",
    "\n",
    "# Function for pivoting summary data\n",
    "def pivot_summary(desc):\n",
    "    desc_p = desc.toPandas()\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT\n",
    "\n",
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)\n",
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "sum_expression = \" + \".join(good_features)\n",
    "\n",
    "# Evaluate precision, recall, and F1-score\n",
    "scored = parsed.fillna(0, subset=good_features).\\\n",
    "                withColumn('score', expr(sum_expression)).\\\n",
    "                select('score', 'is_match')\n",
    "\n",
    "scored.show()\n",
    "\n",
    "def crossTabs(scored: DataFrame, t: float) -> DataFrame:\n",
    "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\\n",
    "        groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\\n",
    "        count()\n",
    "\n",
    "# Confusion matrix and evaluation metrics\n",
    "confused = crossTabs(scored, 4.0)\n",
    "confused.show()\n",
    "\n",
    "confused2 = crossTabs(scored, 2.0)\n",
    "confused2.show()\n",
    "\n",
    "tp = confused.filter(\"above = true\").select(\"true\").collect()[0].true\n",
    "fp = confused.filter(\"above = true\").select(\"false\").collect()[0].false\n",
    "fn = confused.filter(\"above = false\").select(\"true\").fillna(0).collect()[0].true\n",
    "tn = confused.filter(\"above = false\").select(\"false\").collect()[0].false\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
